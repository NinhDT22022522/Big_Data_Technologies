{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "28e62f66-b2a6-4d43-978b-79efaef4acfb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "8b7476cc-1c82-40db-c352-13ef86198862"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "6cb0b604-1fba-4400-9192-4a3f68458757"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "b84c3fdb-79f8-4dd4-f68c-f8be3b7bb315"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "rm: `my_output': No such file or directory\n",
            "2024-04-24 02:19:21,860 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 02:19:22,084 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 02:19:22,084 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 02:19:22,107 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:19:22,438 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 02:19:22,477 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 02:19:22,893 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local416908733_0001\n",
            "2024-04-24 02:19:22,894 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 02:19:23,145 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 02:19:23,147 INFO mapreduce.Job: Running job: job_local416908733_0001\n",
            "2024-04-24 02:19:23,158 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 02:19:23,167 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 02:19:23,175 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:23,175 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:23,237 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 02:19:23,243 INFO mapred.LocalJobRunner: Starting task: attempt_local416908733_0001_m_000000_0\n",
            "2024-04-24 02:19:23,283 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:23,286 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:23,324 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:19:23,342 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:19:23,364 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 02:19:23,470 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 02:19:23,470 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 02:19:23,470 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 02:19:23,471 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 02:19:23,471 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 02:19:23,476 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 02:19:23,483 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-24 02:19:23,492 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 02:19:23,495 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 02:19:23,495 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 02:19:23,496 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 02:19:23,497 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 02:19:23,498 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 02:19:23,502 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 02:19:23,503 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 02:19:23,503 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 02:19:23,504 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 02:19:23,505 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 02:19:23,506 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 02:19:23,565 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 02:19:23,567 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 02:19:23,568 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-24 02:19:23,568 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 02:19:23,572 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:19:23,572 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 02:19:23,572 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 02:19:23,572 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-04-24 02:19:23,572 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-24 02:19:23,582 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 02:19:23,596 INFO mapred.Task: Task:attempt_local416908733_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:19:23,603 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-24 02:19:23,603 INFO mapred.Task: Task 'attempt_local416908733_0001_m_000000_0' done.\n",
            "2024-04-24 02:19:23,610 INFO mapred.Task: Final Counters for attempt_local416908733_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=854181\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=28\n",
            "\t\tTotal committed heap usage (bytes)=418381824\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-24 02:19:23,611 INFO mapred.LocalJobRunner: Finishing task: attempt_local416908733_0001_m_000000_0\n",
            "2024-04-24 02:19:23,612 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 02:19:23,616 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 02:19:23,617 INFO mapred.LocalJobRunner: Starting task: attempt_local416908733_0001_r_000000_0\n",
            "2024-04-24 02:19:23,630 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:23,630 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:23,631 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:19:23,636 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7b7fc712\n",
            "2024-04-24 02:19:23,639 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:19:23,669 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 02:19:23,680 INFO reduce.EventFetcher: attempt_local416908733_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 02:19:23,729 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local416908733_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-04-24 02:19:23,736 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local416908733_0001_m_000000_0\n",
            "2024-04-24 02:19:23,740 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-04-24 02:19:23,745 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 02:19:23,747 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:19:23,747 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 02:19:23,758 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 02:19:23,758 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-24 02:19:23,760 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 02:19:23,761 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-04-24 02:19:23,762 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 02:19:23,762 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 02:19:23,763 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-24 02:19:23,764 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:19:23,773 INFO mapred.Task: Task:attempt_local416908733_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:19:23,775 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:19:23,775 INFO mapred.Task: Task attempt_local416908733_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 02:19:23,777 INFO output.FileOutputCommitter: Saved output of task 'attempt_local416908733_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-24 02:19:23,783 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-24 02:19:23,783 INFO mapred.Task: Task 'attempt_local416908733_0001_r_000000_0' done.\n",
            "2024-04-24 02:19:23,784 INFO mapred.Task: Final Counters for attempt_local416908733_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141992\n",
            "\t\tFILE: Number of bytes written=854231\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=418381824\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-24 02:19:23,784 INFO mapred.LocalJobRunner: Finishing task: attempt_local416908733_0001_r_000000_0\n",
            "2024-04-24 02:19:23,784 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 02:19:24,154 INFO mapreduce.Job: Job job_local416908733_0001 running in uber mode : false\n",
            "2024-04-24 02:19:24,155 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 02:19:24,157 INFO mapreduce.Job: Job job_local416908733_0001 completed successfully\n",
            "2024-04-24 02:19:24,167 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283906\n",
            "\t\tFILE: Number of bytes written=1708412\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=28\n",
            "\t\tTotal committed heap usage (bytes)=836763648\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-24 02:19:24,167 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "a798a3ab-347b-478e-b232-1d7572b08dd4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "c829ba99-caba-4300-d21c-34908a76a088"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-24 02:19 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-04-24 02:19 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "e5b9e9aa-922b-43ae-de28-231dd7c7ca5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Apr 24 02:19 part-00000\n",
            "-rw-r--r-- 1 root root  0 Apr 24 02:19 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "64362df8-0197-4e20-e867-a60c562666af"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "6b4e65b1-36dc-4cad-b5c7-dba0eae6a2ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-24 02:19:29,738 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "06cd8e33-b93a-4a75-909a-9d63326daea5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 02:19:32,173 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-24 02:19:35,187 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 02:19:35,409 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 02:19:35,409 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 02:19:35,442 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:19:35,715 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 02:19:35,745 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 02:19:36,063 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local763389900_0001\n",
            "2024-04-24 02:19:36,063 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 02:19:36,377 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 02:19:36,379 INFO mapreduce.Job: Running job: job_local763389900_0001\n",
            "2024-04-24 02:19:36,400 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 02:19:36,403 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 02:19:36,409 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:36,410 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:36,474 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 02:19:36,480 INFO mapred.LocalJobRunner: Starting task: attempt_local763389900_0001_m_000000_0\n",
            "2024-04-24 02:19:36,512 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:36,513 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:36,550 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:19:36,560 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:19:36,576 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 02:19:36,664 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 02:19:36,664 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 02:19:36,664 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 02:19:36,664 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 02:19:36,664 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 02:19:36,670 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 02:19:36,678 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:19:36,679 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 02:19:36,679 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 02:19:36,679 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-04-24 02:19:36,679 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-24 02:19:36,687 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 02:19:36,726 INFO mapred.Task: Task:attempt_local763389900_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:19:36,731 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:19:36,732 INFO mapred.Task: Task 'attempt_local763389900_0001_m_000000_0' done.\n",
            "2024-04-24 02:19:36,744 INFO mapred.Task: Final Counters for attempt_local763389900_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=852085\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=348127232\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-24 02:19:36,744 INFO mapred.LocalJobRunner: Finishing task: attempt_local763389900_0001_m_000000_0\n",
            "2024-04-24 02:19:36,745 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 02:19:36,750 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 02:19:36,753 INFO mapred.LocalJobRunner: Starting task: attempt_local763389900_0001_r_000000_0\n",
            "2024-04-24 02:19:36,767 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:36,767 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:36,767 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:19:36,781 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3475eb97\n",
            "2024-04-24 02:19:36,784 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:19:36,808 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 02:19:36,811 INFO reduce.EventFetcher: attempt_local763389900_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 02:19:36,861 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local763389900_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-04-24 02:19:36,867 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local763389900_0001_m_000000_0\n",
            "2024-04-24 02:19:36,872 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-04-24 02:19:36,879 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 02:19:36,881 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:19:36,881 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 02:19:36,890 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 02:19:36,890 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-24 02:19:36,892 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 02:19:36,893 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-04-24 02:19:36,893 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 02:19:36,894 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 02:19:36,894 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-24 02:19:36,895 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:19:36,906 INFO mapred.Task: Task:attempt_local763389900_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:19:36,907 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:19:36,907 INFO mapred.Task: Task attempt_local763389900_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 02:19:36,911 INFO output.FileOutputCommitter: Saved output of task 'attempt_local763389900_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-24 02:19:36,913 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-24 02:19:36,913 INFO mapred.Task: Task 'attempt_local763389900_0001_r_000000_0' done.\n",
            "2024-04-24 02:19:36,915 INFO mapred.Task: Final Counters for attempt_local763389900_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142006\n",
            "\t\tFILE: Number of bytes written=852143\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=348127232\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-24 02:19:36,915 INFO mapred.LocalJobRunner: Finishing task: attempt_local763389900_0001_r_000000_0\n",
            "2024-04-24 02:19:36,915 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 02:19:37,387 INFO mapreduce.Job: Job job_local763389900_0001 running in uber mode : false\n",
            "2024-04-24 02:19:37,388 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 02:19:37,389 INFO mapreduce.Job: Job job_local763389900_0001 completed successfully\n",
            "2024-04-24 02:19:37,400 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283920\n",
            "\t\tFILE: Number of bytes written=1704228\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=696254464\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-24 02:19:37,401 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "63c1b408-2c16-481a-aed9-26f9bf2ed254"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "485c5dac-67a2-4965-eb2e-2f3326658a8e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "3203805b-d5ed-4bf6-823e-5bea1e3b8c64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 02:19:41,266 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-24 02:19:43,533 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 02:19:43,720 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 02:19:43,720 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 02:19:43,742 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:19:44,094 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 02:19:44,144 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 02:19:44,729 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local465106827_0001\n",
            "2024-04-24 02:19:44,729 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 02:19:45,220 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 02:19:45,223 INFO mapreduce.Job: Running job: job_local465106827_0001\n",
            "2024-04-24 02:19:45,242 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 02:19:45,245 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 02:19:45,266 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:45,266 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:45,413 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 02:19:45,427 INFO mapred.LocalJobRunner: Starting task: attempt_local465106827_0001_m_000000_0\n",
            "2024-04-24 02:19:45,532 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:45,535 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:45,628 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:19:45,650 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:19:45,680 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-24 02:19:45,741 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:19:45,773 INFO mapred.Task: Task:attempt_local465106827_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:19:45,775 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:19:45,783 INFO mapred.Task: Task attempt_local465106827_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-24 02:19:45,787 INFO output.FileOutputCommitter: Saved output of task 'attempt_local465106827_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-24 02:19:45,796 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:19:45,797 INFO mapred.Task: Task 'attempt_local465106827_0001_m_000000_0' done.\n",
            "2024-04-24 02:19:45,815 INFO mapred.Task: Final Counters for attempt_local465106827_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=852049\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=32\n",
            "\t\tTotal committed heap usage (bytes)=415236096\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-24 02:19:45,818 INFO mapred.LocalJobRunner: Finishing task: attempt_local465106827_0001_m_000000_0\n",
            "2024-04-24 02:19:45,819 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 02:19:46,253 INFO mapreduce.Job: Job job_local465106827_0001 running in uber mode : false\n",
            "2024-04-24 02:19:46,256 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-24 02:19:46,261 INFO mapreduce.Job: Job job_local465106827_0001 completed successfully\n",
            "2024-04-24 02:19:46,271 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=852049\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=32\n",
            "\t\tTotal committed heap usage (bytes)=415236096\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-24 02:19:46,271 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "2c0cc8ea-2020-47bb-e461-29a2a31e6587"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "8d1f93d7-54dd-4138-d0ae-b0a271f18663"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 02:19:50,338 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-24 02:19:52,498 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 02:19:52,728 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 02:19:52,729 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 02:19:52,752 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:19:53,052 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 02:19:53,087 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 02:19:53,432 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1237080587_0001\n",
            "2024-04-24 02:19:53,432 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 02:19:53,726 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 02:19:53,728 INFO mapreduce.Job: Running job: job_local1237080587_0001\n",
            "2024-04-24 02:19:53,743 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 02:19:53,746 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 02:19:53,754 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:53,754 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:53,814 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 02:19:53,820 INFO mapred.LocalJobRunner: Starting task: attempt_local1237080587_0001_m_000000_0\n",
            "2024-04-24 02:19:53,856 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:19:53,859 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:19:53,894 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:19:53,907 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:19:53,926 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-24 02:19:53,943 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-24 02:19:53,956 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 02:19:53,959 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 02:19:53,959 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 02:19:53,960 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 02:19:53,960 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 02:19:53,961 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 02:19:53,962 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 02:19:53,962 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 02:19:53,963 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 02:19:53,963 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 02:19:53,966 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 02:19:53,966 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 02:19:54,020 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 02:19:54,024 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-24 02:19:54,024 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 02:19:54,025 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 02:19:54,029 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:19:54,040 INFO mapred.Task: Task:attempt_local1237080587_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:19:54,042 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:19:54,042 INFO mapred.Task: Task attempt_local1237080587_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-24 02:19:54,045 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1237080587_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-24 02:19:54,046 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-24 02:19:54,046 INFO mapred.Task: Task 'attempt_local1237080587_0001_m_000000_0' done.\n",
            "2024-04-24 02:19:54,058 INFO mapred.Task: Final Counters for attempt_local1237080587_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=858459\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=24\n",
            "\t\tTotal committed heap usage (bytes)=429916160\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-24 02:19:54,058 INFO mapred.LocalJobRunner: Finishing task: attempt_local1237080587_0001_m_000000_0\n",
            "2024-04-24 02:19:54,060 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 02:19:54,740 INFO mapreduce.Job: Job job_local1237080587_0001 running in uber mode : false\n",
            "2024-04-24 02:19:54,742 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-24 02:19:54,746 INFO mapreduce.Job: Job job_local1237080587_0001 completed successfully\n",
            "2024-04-24 02:19:54,755 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=858459\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=24\n",
            "\t\tTotal committed heap usage (bytes)=429916160\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-24 02:19:54,756 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1UDPr6zKKw",
        "outputId": "8428df95-6478-401c-8987-8deb9cd816f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ]
}