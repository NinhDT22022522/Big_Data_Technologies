{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n",
        "\n",
        "# MapReduce: A Primer with <code>Hello World!</code>\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n",
        "\n",
        "> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n",
        "\n",
        "(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n",
        "\n",
        "We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n",
        "\n",
        "> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n",
        "\n",
        "MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n",
        "\n",
        "Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"
      ],
      "metadata": {
        "id": "GzbmlR27wh6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download core Hadoop"
      ],
      "metadata": {
        "id": "uUbM5R0GwwYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n",
        "\n",
        "import requests\n",
        "import os\n",
        "import tarfile\n",
        "\n",
        "def download_and_extract_targz(url):\n",
        "    response = requests.get(url)\n",
        "    filename = url.rsplit('/', 1)[-1]\n",
        "    HADOOP_HOME = filename[:-7]\n",
        "    # set HADOOP_HOME environment variable\n",
        "    os.environ['HADOOP_HOME'] = HADOOP_HOME\n",
        "    if os.path.isdir(HADOOP_HOME):\n",
        "      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n",
        "      return\n",
        "    if response.status_code == 200:\n",
        "        with open(filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        with tarfile.open(filename, 'r:gz') as tar_ref:\n",
        "            extract_path = tar_ref.extractall(path='.')\n",
        "            # Get the names of all members (files and directories) in the archive\n",
        "            all_members = tar_ref.getnames()\n",
        "            # If there is a top-level directory, get its name\n",
        "            if all_members:\n",
        "              top_level_directory = all_members[0]\n",
        "              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n",
        "    else:\n",
        "        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n",
        "\n",
        "\n",
        "download_and_extract_targz(HADOOP_URL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDgQtQlzw8bL",
        "outputId": "1159316a-b534-4093-c2c6-86f85d0b378a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not downloading, Hadoop folder hadoop-3.4.0 already exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set environment variables"
      ],
      "metadata": {
        "id": "3yvb5cw9xEbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `HADOOP_HOME` and `PATH`"
      ],
      "metadata": {
        "id": "u6lkrz1dxIiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HADOOP_HOME was set earlier when downloading Hadoop distribution\n",
        "print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n",
        "\n",
        "os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n",
        "print(\"PATH is {}\".format(os.environ['PATH']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7maAwaFxBT_",
        "outputId": "df1d182c-ce0c-43a7-c3fa-49c2c234ae1d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HADOOP_HOME is hadoop-3.4.0\n",
            "PATH is hadoop-3.4.0/bin:hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set `JAVA_HOME`\n",
        "\n",
        "While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."
      ],
      "metadata": {
        "id": "4kzJ8cNoxPyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# set variable JAVA_HOME (install Java if necessary)\n",
        "def is_java_installed():\n",
        "    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "    return os.environ['JAVA_HOME']\n",
        "\n",
        "def install_java():\n",
        "    # Uncomment and modify the desired version\n",
        "    # java_version= 'openjdk-11-jre-headless'\n",
        "    # java_version= 'default-jre'\n",
        "    # java_version= 'openjdk-17-jre-headless'\n",
        "    # java_version= 'openjdk-18-jre-headless'\n",
        "    java_version= 'openjdk-19-jre-headless'\n",
        "\n",
        "    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n",
        "    try:\n",
        "        cmd = f\"apt install -y {java_version}\"\n",
        "        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "        stdout_result = subprocess_output.stdout\n",
        "        # Process the results as needed\n",
        "        print(\"Done installing Java {}\".format(java_version))\n",
        "        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n",
        "        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # Handle the error if the command returns a non-zero exit code\n",
        "        print(\"Command failed with return code {}\".format(e.returncode))\n",
        "        print(\"stdout: {}\".format(e.stdout))\n",
        "\n",
        "# Install Java if not available\n",
        "if is_java_installed():\n",
        "    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n",
        "else:\n",
        "    print(\"Installing Java\")\n",
        "    install_java()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SauFHVPOxL-Y",
        "outputId": "c9038e34-4556-43a4-82bd-00e0f5c3f616"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a MapReduce job with Hadoop streaming"
      ],
      "metadata": {
        "id": "6HFPVX84xbNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a file\n",
        "\n",
        "Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."
      ],
      "metadata": {
        "id": "_yVa55X1xmOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Hello, World!\">./hello.txt"
      ],
      "metadata": {
        "id": "9Jz7mJkcxYxw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n",
        "\n",
        "Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n",
        "\n",
        "**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."
      ],
      "metadata": {
        "id": "zSh_Kr5Bxvst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb5JryK9xpPA",
        "outputId": "ef58312b-88ae-4594-e0bd-c900860f1eae"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 02:24:07,659 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-24 02:24:09,874 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 02:24:10,047 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 02:24:10,047 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 02:24:10,073 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:24:10,389 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 02:24:10,427 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 02:24:10,763 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1402111943_0001\n",
            "2024-04-24 02:24:10,763 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 02:24:11,053 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 02:24:11,055 INFO mapreduce.Job: Running job: job_local1402111943_0001\n",
            "2024-04-24 02:24:11,064 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 02:24:11,067 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 02:24:11,074 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:11,074 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:11,140 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 02:24:11,160 INFO mapred.LocalJobRunner: Starting task: attempt_local1402111943_0001_m_000000_0\n",
            "2024-04-24 02:24:11,196 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:11,198 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:11,222 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:24:11,235 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:24:11,260 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 02:24:11,353 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 02:24:11,353 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 02:24:11,353 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 02:24:11,353 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 02:24:11,353 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 02:24:11,359 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 02:24:11,363 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-24 02:24:11,377 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 02:24:11,380 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 02:24:11,380 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 02:24:11,381 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 02:24:11,382 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 02:24:11,382 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 02:24:11,384 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 02:24:11,385 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 02:24:11,385 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 02:24:11,386 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 02:24:11,387 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 02:24:11,388 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 02:24:11,417 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 02:24:11,436 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 02:24:11,437 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-24 02:24:11,438 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 02:24:11,442 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:24:11,442 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 02:24:11,442 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 02:24:11,442 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n",
            "2024-04-24 02:24:11,442 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-24 02:24:11,449 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 02:24:11,467 INFO mapred.Task: Task:attempt_local1402111943_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:24:11,470 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-24 02:24:11,470 INFO mapred.Task: Task 'attempt_local1402111943_0001_m_000000_0' done.\n",
            "2024-04-24 02:24:11,479 INFO mapred.Task: Final Counters for attempt_local1402111943_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=857635\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=344981504\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-24 02:24:11,480 INFO mapred.LocalJobRunner: Finishing task: attempt_local1402111943_0001_m_000000_0\n",
            "2024-04-24 02:24:11,481 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 02:24:11,485 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 02:24:11,495 INFO mapred.LocalJobRunner: Starting task: attempt_local1402111943_0001_r_000000_0\n",
            "2024-04-24 02:24:11,507 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:11,507 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:11,508 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:24:11,516 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5064f2f5\n",
            "2024-04-24 02:24:11,518 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:24:11,552 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 02:24:11,562 INFO reduce.EventFetcher: attempt_local1402111943_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 02:24:11,617 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1402111943_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n",
            "2024-04-24 02:24:11,621 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local1402111943_0001_m_000000_0\n",
            "2024-04-24 02:24:11,626 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n",
            "2024-04-24 02:24:11,632 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 02:24:11,633 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:24:11,634 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 02:24:11,642 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 02:24:11,643 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-24 02:24:11,645 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 02:24:11,646 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n",
            "2024-04-24 02:24:11,650 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 02:24:11,650 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 02:24:11,651 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n",
            "2024-04-24 02:24:11,652 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:24:11,671 INFO mapred.Task: Task:attempt_local1402111943_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:24:11,674 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:24:11,674 INFO mapred.Task: Task attempt_local1402111943_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 02:24:11,677 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1402111943_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-24 02:24:11,679 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-24 02:24:11,679 INFO mapred.Task: Task 'attempt_local1402111943_0001_r_000000_0' done.\n",
            "2024-04-24 02:24:11,681 INFO mapred.Task: Final Counters for attempt_local1402111943_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141992\n",
            "\t\tFILE: Number of bytes written=857685\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=344981504\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-24 02:24:11,682 INFO mapred.LocalJobRunner: Finishing task: attempt_local1402111943_0001_r_000000_0\n",
            "2024-04-24 02:24:11,682 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 02:24:12,062 INFO mapreduce.Job: Job job_local1402111943_0001 running in uber mode : false\n",
            "2024-04-24 02:24:12,063 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 02:24:12,064 INFO mapreduce.Job: Job job_local1402111943_0001 completed successfully\n",
            "2024-04-24 02:24:12,075 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283906\n",
            "\t\tFILE: Number of bytes written=1715320\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=15\n",
            "\t\tMap output materialized bytes=23\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=23\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=689963008\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-24 02:24:12,076 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result\n",
        "\n",
        "If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n",
        "\n",
        "Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."
      ],
      "metadata": {
        "id": "OB_fX9u5x55y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnvEvYDfx2g4",
        "outputId": "a0fa2d79-a1b9-4878-b5f3-d30d06cdd56e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."
      ],
      "metadata": {
        "id": "BLMnBh44x_YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -ls my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufAfmGUvx8jW",
        "outputId": "bcca3e79-32c0-49e7-97b9-ce07ca7decd1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2 items\n",
            "-rw-r--r--   1 root root          0 2024-04-24 02:24 my_output/_SUCCESS\n",
            "-rw-r--r--   1 root root         15 2024-04-24 02:24 my_output/part-00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnKSahPzyCAn",
        "outputId": "b5ef53fd-ec7e-47d8-c1d9-954ece408326"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4\n",
            "-rw-r--r-- 1 root root 15 Apr 24 02:24 part-00000\n",
            "-rw-r--r-- 1 root root  0 Apr 24 02:24 _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."
      ],
      "metadata": {
        "id": "v9LmpcaMyG23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-Clat5yD8I",
        "outputId": "8e9cd9e9-8e6d-425d-9a02-55743b115f30"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MapReduce without specifying mapper or reducer\n",
        "\n",
        "In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n",
        "\n",
        "Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."
      ],
      "metadata": {
        "id": "AmpHr_HyyMnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mapred streaming -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPWL1AiXyJac",
        "outputId": "19c28f1a-238c-4811-b9aa-9be68569525f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-24 02:24:19,086 ERROR streaming.StreamJob: Unrecognized option: -h\n",
            "Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n",
            "Options:\n",
            "  -input          <path> DFS input file(s) for the Map step.\n",
            "  -output         <path> DFS output directory for the Reduce step.\n",
            "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
            "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
            "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
            "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
            "                  Deprecated. Use generic option \"-files\" instead.\n",
            "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
            "                  Optional. The input format class.\n",
            "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
            "                  Optional. The output format class.\n",
            "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
            "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
            "  -inputreader    <spec> Optional. Input recordreader spec.\n",
            "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
            "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
            "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
            "  -io             <identifier> Optional. Format to use for input to and output\n",
            "                  from mapper/reducer commands\n",
            "  -lazyOutput     Optional. Lazily create Output.\n",
            "  -background     Optional. Submit the job and don't wait till it completes.\n",
            "  -verbose        Optional. Print verbose output.\n",
            "  -info           Optional. Print detailed usage.\n",
            "  -help           Optional. Print help message.\n",
            "\n",
            "Generic options supported are:\n",
            "-conf <configuration file>        specify an application configuration file\n",
            "-D <property=value>               define a value for a given property\n",
            "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
            "-jt <local|resourcemanager:port>  specify a ResourceManager\n",
            "-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n",
            "-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n",
            "-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n",
            "\n",
            "The general command line syntax is:\n",
            "command [genericOptions] [commandOptions]\n",
            "\n",
            "\n",
            "For more details about these options:\n",
            "Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n",
            "\n",
            "Try -help for more information\n",
            "Streaming Command Failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5H2MkIUPyQc2",
        "outputId": "99d7c0c8-c3af-40fd-dc02-5ab0a60c6a93"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 02:24:21,139 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-24 02:24:23,466 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 02:24:23,654 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 02:24:23,655 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 02:24:23,682 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:24:23,964 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 02:24:24,002 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 02:24:24,320 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2068825590_0001\n",
            "2024-04-24 02:24:24,320 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 02:24:24,611 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 02:24:24,613 INFO mapreduce.Job: Running job: job_local2068825590_0001\n",
            "2024-04-24 02:24:24,624 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 02:24:24,628 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 02:24:24,643 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:24,643 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:24,718 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 02:24:24,724 INFO mapred.LocalJobRunner: Starting task: attempt_local2068825590_0001_m_000000_0\n",
            "2024-04-24 02:24:24,770 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:24,771 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:24,796 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:24:24,806 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:24:24,827 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2024-04-24 02:24:24,920 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2024-04-24 02:24:24,920 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2024-04-24 02:24:24,920 INFO mapred.MapTask: soft limit at 83886080\n",
            "2024-04-24 02:24:24,920 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2024-04-24 02:24:24,920 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2024-04-24 02:24:24,926 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2024-04-24 02:24:24,937 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:24:24,938 INFO mapred.MapTask: Starting flush of map output\n",
            "2024-04-24 02:24:24,938 INFO mapred.MapTask: Spilling map output\n",
            "2024-04-24 02:24:24,938 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n",
            "2024-04-24 02:24:24,938 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n",
            "2024-04-24 02:24:24,965 INFO mapred.MapTask: Finished spill 0\n",
            "2024-04-24 02:24:24,993 INFO mapred.Task: Task:attempt_local2068825590_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:24:24,999 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:24:25,000 INFO mapred.Task: Task 'attempt_local2068825590_0001_m_000000_0' done.\n",
            "2024-04-24 02:24:25,010 INFO mapred.Task: Final Counters for attempt_local2068825590_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855525\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=395313152\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "2024-04-24 02:24:25,015 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068825590_0001_m_000000_0\n",
            "2024-04-24 02:24:25,017 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 02:24:25,022 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2024-04-24 02:24:25,023 INFO mapred.LocalJobRunner: Starting task: attempt_local2068825590_0001_r_000000_0\n",
            "2024-04-24 02:24:25,037 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:25,037 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:25,038 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:24:25,046 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64c5b0a4\n",
            "2024-04-24 02:24:25,049 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:24:25,087 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2024-04-24 02:24:25,093 INFO reduce.EventFetcher: attempt_local2068825590_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2024-04-24 02:24:25,141 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2068825590_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n",
            "2024-04-24 02:24:25,150 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local2068825590_0001_m_000000_0\n",
            "2024-04-24 02:24:25,156 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n",
            "2024-04-24 02:24:25,162 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2024-04-24 02:24:25,164 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:24:25,164 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2024-04-24 02:24:25,173 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 02:24:25,173 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-24 02:24:25,175 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n",
            "2024-04-24 02:24:25,176 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n",
            "2024-04-24 02:24:25,177 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2024-04-24 02:24:25,177 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2024-04-24 02:24:25,180 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n",
            "2024-04-24 02:24:25,181 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:24:25,202 INFO mapred.Task: Task:attempt_local2068825590_0001_r_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:24:25,207 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2024-04-24 02:24:25,207 INFO mapred.Task: Task attempt_local2068825590_0001_r_000000_0 is allowed to commit now\n",
            "2024-04-24 02:24:25,211 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2068825590_0001_r_000000_0' to file:/content/my_output\n",
            "2024-04-24 02:24:25,212 INFO mapred.LocalJobRunner: reduce > reduce\n",
            "2024-04-24 02:24:25,212 INFO mapred.Task: Task 'attempt_local2068825590_0001_r_000000_0' done.\n",
            "2024-04-24 02:24:25,213 INFO mapred.Task: Final Counters for attempt_local2068825590_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=142006\n",
            "\t\tFILE: Number of bytes written=855583\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=1\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=395313152\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-24 02:24:25,213 INFO mapred.LocalJobRunner: Finishing task: attempt_local2068825590_0001_r_000000_0\n",
            "2024-04-24 02:24:25,214 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2024-04-24 02:24:25,621 INFO mapreduce.Job: Job job_local2068825590_0001 running in uber mode : false\n",
            "2024-04-24 02:24:25,622 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2024-04-24 02:24:25,623 INFO mapreduce.Job: Job job_local2068825590_0001 completed successfully\n",
            "2024-04-24 02:24:25,643 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=283920\n",
            "\t\tFILE: Number of bytes written=1711108\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tMap output bytes=22\n",
            "\t\tMap output materialized bytes=30\n",
            "\t\tInput split bytes=75\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=1\n",
            "\t\tReduce shuffle bytes=30\n",
            "\t\tReduce input records=1\n",
            "\t\tReduce output records=1\n",
            "\t\tSpilled Records=2\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=15\n",
            "\t\tTotal committed heap usage (bytes)=790626304\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-24 02:24:25,643 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "v7Ks3e96yXuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "echo \"Check if MapReduce job was successful\"\n",
        "hdfs dfs -test -e my_output/_SUCCESS\n",
        "if [ $? -eq 0 ]; then\n",
        "\techo \"_SUCCESS exists!\"\n",
        "fi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWAXvG0_yThc",
        "outputId": "2b22baaf-8756-4655-b289-e9f7e1cf16a8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check if MapReduce job was successful\n",
            "_SUCCESS exists!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show output"
      ],
      "metadata": {
        "id": "t40GgJ2Hya9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5APWEgoyaRS",
        "outputId": "b6bbd535-458d-4334-f9ad-3bd06240eb17"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."
      ],
      "metadata": {
        "id": "mzfaMVKqyjpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run a map-only MapReduce job\n",
        "\n",
        "Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n",
        "\n",
        "To run a MapReduce job _without_ reducer one needs to use the generic option\n",
        "\n",
        "    \\-D mapreduce.job.reduces=0\n",
        "\n",
        "(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."
      ],
      "metadata": {
        "id": "lzIuWv7Myndc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdwKWyVRye27",
        "outputId": "d82d39aa-0a07-4292-f9e1-219bbcd1014b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 02:24:30,838 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-24 02:24:33,147 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 02:24:33,365 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 02:24:33,365 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 02:24:33,391 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:24:33,653 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 02:24:33,685 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 02:24:34,029 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1108696547_0001\n",
            "2024-04-24 02:24:34,029 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 02:24:34,282 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 02:24:34,284 INFO mapreduce.Job: Running job: job_local1108696547_0001\n",
            "2024-04-24 02:24:34,294 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 02:24:34,297 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 02:24:34,307 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:34,307 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:34,366 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 02:24:34,373 INFO mapred.LocalJobRunner: Starting task: attempt_local1108696547_0001_m_000000_0\n",
            "2024-04-24 02:24:34,428 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:34,429 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:34,468 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:24:34,483 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:24:34,507 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-24 02:24:34,534 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:24:34,551 INFO mapred.Task: Task:attempt_local1108696547_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:24:34,552 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:24:34,553 INFO mapred.Task: Task attempt_local1108696547_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-24 02:24:34,560 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1108696547_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-24 02:24:34,561 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:24:34,562 INFO mapred.Task: Task 'attempt_local1108696547_0001_m_000000_0' done.\n",
            "2024-04-24 02:24:34,568 INFO mapred.Task: Final Counters for attempt_local1108696547_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855489\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-24 02:24:34,568 INFO mapred.LocalJobRunner: Finishing task: attempt_local1108696547_0001_m_000000_0\n",
            "2024-04-24 02:24:34,570 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 02:24:35,291 INFO mapreduce.Job: Job job_local1108696547_0001 running in uber mode : false\n",
            "2024-04-24 02:24:35,294 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-24 02:24:35,298 INFO mapreduce.Job: Job job_local1108696547_0001 completed successfully\n",
            "2024-04-24 02:24:35,305 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855489\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=357564416\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=28\n",
            "2024-04-24 02:24:35,305 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify the result"
      ],
      "metadata": {
        "id": "QZIE9yXOyyHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dt3tUI0yu5e",
        "outputId": "765e206e-dc4b-490f-ed91-30f3d3bdff06"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tHello, World!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why a map-only application?\n",
        "\n",
        "The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n",
        "\n",
        "On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"
      ],
      "metadata": {
        "id": "hUGEUv99y3cM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved version of the MapReduce \"Hello, World!\" application\n",
        "\n",
        "Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."
      ],
      "metadata": {
        "id": "FhVVFEdKzGcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "hdfs dfs -rm -r my_output\n",
        "\n",
        "mapred streaming \\\n",
        "    -D mapreduce.job.reduces=0 \\\n",
        "    -input hello.txt \\\n",
        "    -output my_output \\\n",
        "    -mapper '/bin/cat'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLgMXX2jy0vC",
        "outputId": "9e9c961c-cc7a-4b4c-ccc6-70450009b1c7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted my_output\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-04-24 02:24:39,221 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n",
            "2024-04-24 02:24:42,255 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2024-04-24 02:24:42,557 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2024-04-24 02:24:42,557 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2024-04-24 02:24:42,604 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2024-04-24 02:24:43,054 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2024-04-24 02:24:43,150 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2024-04-24 02:24:43,682 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local732706030_0001\n",
            "2024-04-24 02:24:43,682 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2024-04-24 02:24:43,963 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2024-04-24 02:24:43,965 INFO mapreduce.Job: Running job: job_local732706030_0001\n",
            "2024-04-24 02:24:43,976 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2024-04-24 02:24:43,979 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2024-04-24 02:24:43,988 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:43,988 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:44,064 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2024-04-24 02:24:44,078 INFO mapred.LocalJobRunner: Starting task: attempt_local732706030_0001_m_000000_0\n",
            "2024-04-24 02:24:44,142 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2024-04-24 02:24:44,142 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2024-04-24 02:24:44,180 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2024-04-24 02:24:44,192 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n",
            "2024-04-24 02:24:44,211 INFO mapred.MapTask: numReduceTasks: 0\n",
            "2024-04-24 02:24:44,230 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n",
            "2024-04-24 02:24:44,242 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2024-04-24 02:24:44,246 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2024-04-24 02:24:44,247 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2024-04-24 02:24:44,248 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2024-04-24 02:24:44,249 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2024-04-24 02:24:44,249 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2024-04-24 02:24:44,251 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2024-04-24 02:24:44,251 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2024-04-24 02:24:44,252 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2024-04-24 02:24:44,252 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2024-04-24 02:24:44,253 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2024-04-24 02:24:44,254 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2024-04-24 02:24:44,284 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2024-04-24 02:24:44,292 INFO streaming.PipeMapRed: Records R/W=1/1\n",
            "2024-04-24 02:24:44,293 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2024-04-24 02:24:44,293 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2024-04-24 02:24:44,296 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:24:44,309 INFO mapred.Task: Task:attempt_local732706030_0001_m_000000_0 is done. And is in the process of committing\n",
            "2024-04-24 02:24:44,310 INFO mapred.LocalJobRunner: \n",
            "2024-04-24 02:24:44,311 INFO mapred.Task: Task attempt_local732706030_0001_m_000000_0 is allowed to commit now\n",
            "2024-04-24 02:24:44,313 INFO output.FileOutputCommitter: Saved output of task 'attempt_local732706030_0001_m_000000_0' to file:/content/my_output\n",
            "2024-04-24 02:24:44,316 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
            "2024-04-24 02:24:44,316 INFO mapred.Task: Task 'attempt_local732706030_0001_m_000000_0' done.\n",
            "2024-04-24 02:24:44,325 INFO mapred.Task: Final Counters for attempt_local732706030_0001_m_000000_0: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=14\n",
            "\t\tTotal committed heap usage (bytes)=378535936\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-24 02:24:44,325 INFO mapred.LocalJobRunner: Finishing task: attempt_local732706030_0001_m_000000_0\n",
            "2024-04-24 02:24:44,326 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2024-04-24 02:24:44,973 INFO mapreduce.Job: Job job_local732706030_0001 running in uber mode : false\n",
            "2024-04-24 02:24:44,975 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2024-04-24 02:24:44,979 INFO mapreduce.Job: Job job_local732706030_0001 completed successfully\n",
            "2024-04-24 02:24:44,991 INFO mapreduce.Job: Counters: 15\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141914\n",
            "\t\tFILE: Number of bytes written=855001\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=1\n",
            "\t\tMap output records=1\n",
            "\t\tInput split bytes=75\n",
            "\t\tSpilled Records=0\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=14\n",
            "\t\tTotal committed heap usage (bytes)=378535936\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=14\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=27\n",
            "2024-04-24 02:24:44,991 INFO streaming.StreamJob: Output directory: my_output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa1UDPr6zKKw",
        "outputId": "bcd9e009-f0a3-4b69-adc2-bf27be4bd2bc"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, World!\t\n"
          ]
        }
      ]
    }
  ]
}