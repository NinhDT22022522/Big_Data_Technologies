{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NinhDT22022522/Big_Data_Technologies/blob/main/demoSparkSQLPython.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aztkOI9IDsU"
      },
      "source": [
        "# Getting started with Spark: Spark SQL in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iirqfdnVIDsW"
      },
      "source": [
        "This tutorial is based on [Spark SQL Guide - Getting started](https://spark.apache.org/docs/latest/sql-getting-started.html).\n",
        "\n",
        "For this demo we used the city of Vienna trees dataset (\"Baumkataster\") made available by [Open Data Ã–sterreich](https://www.data.gv.at) and downloadable from [here](https://www.data.gv.at/katalog/dataset/c91a4635-8b7d-43fe-9b27-d95dec8392a7) .\n",
        "\n",
        "# Table of contents\n",
        "1. [Spark session](#sparkSession)\n",
        "2. [Spark configuration](#sparkConfiguration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUMZPZp-IDsW"
      },
      "source": [
        "## Spark session <a name=\"sparkSession\"></a>\n",
        "\n",
        "We're going to start by creating a Spark _session_. Our Spark job will be named \"Python Spark SQL basic example\". `spark` is the variable holding our Spark session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CUaYe0baIDsX",
        "outputId": "2121a262-c3a3-4c9a-d3b0-3c3769ab6b15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-651c94717461>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Python Spark SQL basic example\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUDOPLIzIDsX"
      },
      "source": [
        "Read the file into a Spark [_dataframe_](https://spark.apache.org/docs/latest/sql-programming-guide.html#datasets-and-dataframes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LrrGj7-IDsY"
      },
      "outputs": [],
      "source": [
        "df = spark.read \\\n",
        "          .load(\"FME_BaumdatenBearbeitet_OGD_20190205.csv\",\n",
        "           format=\"csv\", sep=\";\", header=\"true\", encoding=\"iso-8859-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBFXC8xXIDsY"
      },
      "source": [
        "**Note:** we assume that the file `FME_BaumdatenBearbeitet_OGD_20190205.csv` is in your local directory. If at this point you get an error message that looks like `AnalysisException: 'Path does not exist` then check your [Spark configuration](#sparkConfig) for how to define the correct file path."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4IreEHVIDsY"
      },
      "source": [
        "Show first three lines of Spark dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uPzQDLFIDsY"
      },
      "outputs": [],
      "source": [
        "df.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1my9UtDIDsZ"
      },
      "source": [
        "For pretty-printing you can use `toPandas()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYzF0icTIDsZ"
      },
      "outputs": [],
      "source": [
        "df.toPandas().head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOOLlXgdIDsZ"
      },
      "source": [
        "Show number of different trees (count German names in `df` and sort by count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKDJ2gZYIDsZ"
      },
      "outputs": [],
      "source": [
        "df.groupBy(\"NameDeutsch\").count().orderBy('count', ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TFVFBODIDsa"
      },
      "source": [
        "An example of SQL query (see [Running SQL Queries Programmatically](https://spark.apache.org/docs/latest/sql-getting-started.html#running-sql-queries-programmatically)): let's sort trees by height (\"Hoehe\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awQ-T_h2IDsa"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"baeume\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8TsNIHwIDsa"
      },
      "outputs": [],
      "source": [
        "spark.sql(\"SELECT BaumNr, NameDeutsch, Hoehe, lat, lon FROM baeume order  by Hoehe desc\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EY04SeKIDsa"
      },
      "source": [
        "The height data doesn't seem to be up-to-date."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTUPvfCoIDsb"
      },
      "source": [
        "## Spark configuration <a name=\"sparkConfiguration\"></a>\n",
        "\n",
        "Spark properties control most application settings and are configured separately for each application. These properties can be set directly on a `SparkConf` passed to your `SparkContext` (from [Apache Spark documentation](https://spark.apache.org/docs/latest/configuration.html#spark-properties)).  \n",
        "\n",
        "We've already seen how to modify the `SparkConf` when we created our Spark application session with the command:\n",
        "<pre>\n",
        "    spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .getOrCreate()\n",
        "</pre>\n",
        "\n",
        "Let us look at the rest of the Spark configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mg6Ebvn0IDsb"
      },
      "outputs": [],
      "source": [
        "from pyspark.conf import SparkConf\n",
        "spark.sparkContext._conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7c8Q_rMIDsb"
      },
      "source": [
        "The property `spark.app.name` is the name of our app that we just defined.\n",
        "\n",
        "Another important property is `spark.master`. This defines the _master URL_  for the Spark application. A list of all admissible values for `spark.master` is given here: [master-urls](https://spark.apache.org/docs/latest/submitting-applications.html#master-urls).\n",
        "\n",
        "In this example the Spark master URL is `local[*]`, this means that our Spark application will run locally with as many worker threads as logical cores on our local machine.\n",
        "\n",
        "If you have a Hadoop cluster available you can deploy your Spark application on Yarn by setting the option `spark.master = yarn`. Let's do that and then check the Spark configuration once again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0bTMKOdIDsb"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"Python Spark SQL basic example\") \\\n",
        "    .master('yarn') \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark.sparkContext._conf.getAll()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TiAd5BNIDsc"
      },
      "source": [
        "With this configuration our Spark application will run on the Hadoop cluster and its resources will be managed by Yarn.\n",
        "\n",
        "**Note:** If the Hadoop cluster is configured with HDFS as its default filesystem, then you need to upload your CSV file to Hadoop in order to be able to read it:\n",
        "<code>\n",
        "    hdfs dfs -put FME_BaumdatenBearbeitet_OGD_20190205.csv FME_BaumdatenBearbeitet_OGD_20190205.csv\n",
        "</code>\n",
        "and then you can just use `.load( ...) ` again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AF03zxhIDsc"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "hdfs dfs -put FME_BaumdatenBearbeitet_OGD_20190205.csv\n",
        "hdfs dfs -ls FME_BaumdatenBearbeitet_OGD_20190205.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3f-gGx0IDsc"
      },
      "outputs": [],
      "source": [
        "df = spark.read \\\n",
        "          .load(\"FME_BaumdatenBearbeitet_OGD_20190205.csv\",\n",
        "           format=\"csv\", sep=\";\", header=\"true\", encoding=\"iso-8859-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWerXDqLIDsc"
      },
      "source": [
        "Let's now re-run the previous commands. This time the application is going to be deployed on the cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SII23k41IDsc"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"baeume\")\n",
        "spark.sql(\"SELECT BaumNr, NameDeutsch, Hoehe, lat, lon FROM baeume order  by Hoehe desc\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI2gWtykIDsd"
      },
      "source": [
        "**Note:** After you're done, it's important to close the Spark session in order to release cluster resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkqsihJGIDsd"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}